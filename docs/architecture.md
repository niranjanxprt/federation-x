# System Architecture

This document describes the technical architecture of Federation-X.

## Overview

Federation-X is built on the **Flower Federated Learning Framework** with a client-server architecture where hospitals act as federated clients and a central server aggregates their model updates.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEDERATED LEARNING SYSTEM                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Hospital A   â”‚    â”‚ Hospital B   â”‚    â”‚ Hospital C   â”‚       â”‚
â”‚  â”‚ (Client)     â”‚    â”‚ (Client)     â”‚    â”‚ (Client)     â”‚       â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚              â”‚       â”‚
â”‚  â”‚ â€¢ Local Data â”‚    â”‚ â€¢ Local Data â”‚    â”‚ â€¢ Local Data â”‚       â”‚
â”‚  â”‚ â€¢ Local GPU  â”‚    â”‚ â€¢ Local GPU  â”‚    â”‚ â€¢ Local GPU  â”‚       â”‚
â”‚  â”‚ â€¢ Local Trainâ”‚    â”‚ â€¢ Local Trainâ”‚    â”‚ â€¢ Local Trainâ”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                    â”‚                    â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                              â”‚                                    â”‚
â”‚                        Model Updates                             â”‚
â”‚                        (Encrypted)                               â”‚
â”‚                              â”‚                                    â”‚
â”‚                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                              â”‚
â”‚                         â”‚ Server   â”‚                              â”‚
â”‚                         â”‚(Central) â”‚                              â”‚
â”‚                         â”‚          â”‚                              â”‚
â”‚                         â”‚ â€¢ Fetch  â”‚                              â”‚
â”‚                         â”‚ â€¢ Agg.   â”‚                              â”‚
â”‚                         â”‚ â€¢ Eval.  â”‚                              â”‚
â”‚                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                               â”‚
â”‚                              â”‚                                    â”‚
â”‚                         Updated Model                            â”‚
â”‚                              â”‚                                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚                    â”‚                    â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚Hospital A    â”‚    â”‚ Hospital B   â”‚    â”‚ Hospital C   â”‚      â”‚
â”‚  â”‚ (Next Round) â”‚    â”‚ (Next Round) â”‚    â”‚ (Next Round) â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Components

### 1. Server Application (`server_app.py`)

**Responsibility**: Orchestrates federated training and model aggregation

**Key Functions**:
- **Initialization**: Creates global model and Flower server
- **Client Selection**: Selects available hospitals for training
- **Aggregation**: Combines hospital updates using FedProx
- **Evaluation**: Evaluates aggregated model on hospital test sets
- **Checkpointing**: Saves best models and intermediate states
- **Logging**: Tracks metrics to Weights & Biases

**Strategy**: FedProx with configurable proximal term (Î¼)
```python
# FedProx penalty term
L_prox = (Î¼ / 2) * ||w - w_t||Â²
```

**Aggregation Logic**:
```
For each round t:
  1. Select K hospitals from M total
  2. Send current model w_t to selected hospitals
  3. Wait for hospitals to train locally
  4. Receive updated weights w_t^i from each hospital
  5. Aggregate: w_{t+1} = (1/K) * Î£ w_t^i
  6. Evaluate on test sets
  7. Log metrics
  8. Save checkpoint
```

### 2. Client Application (`client_app.py`)

**Responsibility**: Represents a hospital in federated training

**Key Functions**:
- **Model Download**: Receives aggregated model from server
- **Local Training**: Trains on local hospital data
- **Gradient Computation**: Calculates weight updates
- **Model Upload**: Sends updated weights to server
- **Evaluation**: Evaluates on local test set

**Data Pipeline**:
```
Hospital Data
    â†“
Data Loading (128x128 images)
    â†“
Preprocessing (normalization)
    â†“
Mini-batch Creation
    â†“
Training Loop
    â†“
Weight Update
```

### 3. Task Module (`task.py`)

**Responsibility**: Core ML components (model, training, evaluation)

**Key Classes**:

#### Net (Model Architecture)
```python
class Net(nn.Module):
    """
    ResNet18-based model for binary classification

    Architecture:
    - Input: 128x128 grayscale X-ray
    - Backbone: ResNet18 (pretrained on ImageNet)
    - Head: Linear(512 â†’ 1) for binary classification

    Forward pass:
    X-ray â†’ Conv layers â†’ Feature extraction â†’ Classification
    """
```

**Key Design Decisions**:
- **Pre-trained Weights**: ImageNet weights for faster convergence
- **Grayscale Input**: 1 channel for X-rays (adapted from 3-channel RGB)
- **Binary Output**: Single neuron with sigmoid for pathology presence

#### Training Loop
```python
def train(net, trainloader, epochs, lr, device):
    """
    Local training at hospital

    Algorithm:
    For each epoch:
      For each batch (X, y):
        1. Forward pass: y_pred = model(X)
        2. Compute loss: L = FocalLoss(y_pred, y)
        3. Backward: âˆ‚L/âˆ‚w
        4. Optimizer step: w := w - lr * âˆ‚L/âˆ‚w
        5. Update learning rate scheduler

    Returns: Average loss over all batches
    """
```

**Loss Function**: Focal Loss
```
FL(pt) = -Î±(1-pt)^Î³ * log(pt)

Where:
- pt: probability of correct class
- Î±: weighting factor (0.25 for positives)
- Î³: focusing parameter (2.0)

Benefit: Down-weights easy examples, focuses on hard negatives
```

**Optimizer**: AdamW
```python
AdamW(lr=0.01, weight_decay=0.01)
```

#### Evaluation Function
```python
def evaluate(net, testloader, device):
    """
    Compute AUROC and other metrics

    Metrics:
    - AUROC: Primary metric
    - Sensitivity: True positive rate
    - Specificity: True negative rate
    - Accuracy: Overall correctness
    """
```

### 4. Utilities (`util.py`)

**Utility Functions**:
- Data loading and caching
- Preprocessing pipelines
- Metric computation
- Logging helpers
- Device management

---

## Data Flow

### Training Round Flow

```
Round t:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Server broadcasts model w_t   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
      â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”
â”‚Hospital â”‚   â”‚Hospital â”‚   â”‚Hospital â”‚
â”‚    A    â”‚   â”‚    B    â”‚   â”‚    C    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Train   â”‚   â”‚ Train   â”‚   â”‚ Train   â”‚
â”‚w_t â†’ w'â”‚   â”‚w_t â†’ w''â”‚   â”‚w_t â†’ w'''
â”‚ A      â”‚   â”‚ B       â”‚   â”‚ C       â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
      â”‚             â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ 2. Send updates     â”‚
      â”‚ (weights only)      â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ 3. Aggregate        â”‚
      â”‚ w_{t+1} = Avg(w'_A, â”‚
      â”‚           w'_B, w'_C)
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ 4. Evaluate global  â”‚
      â”‚ model on all tests  â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ 5. Log metrics & CP â”‚
      â”‚ (W&B, disk)         â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Communication Protocol

### Message Format

**Client â†’ Server** (Model Updates):
```
Frame {
  hospital_id: str
  round: int
  weights: dict[str, tensor]
  num_examples: int
  metrics: dict
}
```

**Server â†’ Client** (Model Weights):
```
Frame {
  round: int
  weights: dict[str, tensor]
  config: dict
}
```

### Privacy Considerations

- âœ… **Patient data stays local**: Never transmitted
- âœ… **Only model updates shared**: Weights and gradients only
- âœ… **Aggregation on server**: No hospital sees others' gradients
- âš ï¸ **Current**: No additional encryption (future improvement)
- ğŸ”’ **Recommended**: Add secure aggregation or differential privacy

---

## Federated Averaging (FedAvg)

### Algorithm

```
Input: number of rounds T, clients K
Output: global model w

Initialize: w_0 randomly

For t = 0 to T-1:

  // Server selects clients
  S_t â† random sample of K hospitals

  // Clients train locally
  For each hospital i in S_t (in parallel):
    w_t^i â† ClientUpdate(i, w_t)

  // Server aggregates
  w_{t+1} â† (1/K) * Î£_{i âˆˆ S_t} w_t^i

  // Server evaluates
  metrics_t â† Evaluate(w_{t+1})
```

### FedProx Extension

We use **FedProx** which adds a regularization term to prevent hospitals from drifting too far from the global model:

```
Hospital local loss:
L_i(w) = original_loss(w) + (Î¼/2) * ||w - w_t||Â²

Effect: Keeps hospitals' updates closer to global model
Benefit: Better convergence with heterogeneous data
```

---

## File Structure

```
cold_start_hackathon/
â”œâ”€â”€ __init__.py                # Package initialization
â”œâ”€â”€ server_app.py              # Flower ServerApp
â”‚   â”œâ”€â”€ HackathonFedProx       # Custom FedProx strategy
â”‚   â”œâ”€â”€ main()                 # Server initialization
â”‚   â””â”€â”€ Checkpointing          # Model persistence
â”œâ”€â”€ client_app.py              # Flower ClientApp
â”‚   â”œâ”€â”€ Client class           # Implements ClientApp
â”‚   â”œâ”€â”€ fit()                  # Local training
â”‚   â””â”€â”€ evaluate()             # Local evaluation
â”œâ”€â”€ task.py                    # ML components
â”‚   â”œâ”€â”€ Net                    # Model architecture
â”‚   â”œâ”€â”€ train()                # Training loop
â”‚   â”œâ”€â”€ evaluate()             # Evaluation metrics
â”‚   â””â”€â”€ load_data()            # Data loading
â”œâ”€â”€ util.py                    # Utilities
â”‚   â”œâ”€â”€ Preprocessing          # Image normalization
â”‚   â”œâ”€â”€ Metrics                # Evaluation metrics
â”‚   â””â”€â”€ Logging                # W&B integration
â””â”€â”€ losses.py                  # Custom loss functions
    â”œâ”€â”€ FocalLoss              # Handles class imbalance
    â””â”€â”€ AdaptiveFocalLoss      # Dynamic weighting
```

---

## Distributed Execution

### Local Simulation
```bash
flwr run . local
# Spawns 3 virtual clients locally
# Ideal for testing on single machine
```

### Cluster Deployment
```bash
flwr run . cluster
# Hospitals run as separate SLURM jobs
# Each hospital: 1 GPU, 2 vCPUs, 32GB RAM
# Up to 4 parallel hospitals
```

### Compute Resources

| Component | Resource | Details |
|-----------|----------|---------|
| **Server** | 1 vCPU, 4GB RAM | Central aggregation |
| **Client** | 2 vCPUs, 32GB RAM | Training + data loading |
| **GPU** | 1Ã— NVIDIA GPU | Shared among clients |
| **Storage** | 5GB | Model + data |

---

## Performance Characteristics

### Timing per Round

```
Round breakdown (20-min constraint):
â”œâ”€ Data loading:        5-10 seconds
â”œâ”€ 3 hospitals train:    2-3 minutes each (parallel)
â”œâ”€ Model aggregation:    10-20 seconds
â”œâ”€ Evaluation:           20-30 seconds
â”œâ”€ Checkpointing:        10-15 seconds
â””â”€ Total:                ~3-4 minutes per round

âŸ¹ Fits 9 rounds in 20 minutes
```

### Scalability

| Aspect | Current | Maximum |
|--------|---------|---------|
| **Hospitals** | 3 | Limited by cluster |
| **Parallel Clients** | 4 | 4 (cluster limit) |
| **Images/Hospital** | ~20-40K | Limited by disk |
| **Model Size** | ~40-50MB | Depends on architecture |

---

## Error Handling & Resilience

### Client Failures
```python
# Server tolerates missing hospitals
min_available_clients = 2  # Can proceed with 2 of 3

# Recovers from:
- Network timeouts
- Training failures
- Evaluation errors
```

### Checkpointing
```python
# Automatic saves every round
checkpoint = {
    'model_weights': state_dict,
    'round': round_number,
    'metrics': evaluation_results,
    'timestamp': current_time
}
```

### Disaster Recovery
```bash
# Resume from checkpoint
./submit-job.sh "flwr run . cluster --checkpoint latest"
```

---

## Monitoring & Logging

### Metrics Tracked

**Training Metrics**:
- Loss per round
- Learning rate
- Gradient norms

**Evaluation Metrics**:
- AUROC (all hospitals)
- Sensitivity / Specificity
- Per-hospital breakdown

**System Metrics**:
- Round duration
- Communication overhead
- GPU/CPU utilization

### Logging Backends

```
Logs â†’ Weights & Biases (cloud)
     â†’ Local files (./logs/)
     â†’ Console output (terminal)
```

---

## Future Architecture Improvements

- [ ] Asynchronous aggregation (FedAsync)
- [ ] Differential privacy integration
- [ ] Secure multi-party computation
- [ ] Knowledge distillation for compression
- [ ] Personalized federated learning
- [ ] Horizontal + vertical partitioning

---

## References

- [Flower Framework Docs](https://flower.ai/)
- [FedAvg Paper](https://arxiv.org/pdf/1602.05629.pdf)
- [FedProx Paper](https://arxiv.org/pdf/1812.06127.pdf)
- [Focal Loss Paper](https://arxiv.org/pdf/1708.02002.pdf)

---

[â† Back Home](index.md) | [Getting Started](./getting-started.md) | [Training Guide â†’](./training-guide.md)
